{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../fairseq-l-system-captioning/')\n",
    "import torch\n",
    "from torch import nn\n",
    "from hhi_pl_utils import Experiment\n",
    "from typing import Union, List\n",
    "\n",
    "from pycore.blocks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\magnusson\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "e = Experiment()\n",
    "e.setup('../fairseq-l-system-captioning/configs/simplest/slim_lstm_normal.yml')\n",
    "e.init_classes()\n",
    "e.data.setup()\n",
    "\n",
    "e.model.vocab = e.data.vocab\n",
    "\n",
    "model = e.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lsystem_modules.slim_lstm.SlimLstm'>\n",
      "<class 'seq2seq.encoder.slim.SlimCNN'>\n",
      "<class 'seq2seq.decoder.lstm.LSTM'>\n",
      "<class 'torch.nn.modules.conv.LazyConv2d'>\n",
      "<class 'torch.nn.modules.batchnorm.LazyBatchNorm2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.nn.modules.conv.LazyConv2d'>\n",
      "<class 'torch.nn.modules.batchnorm.LazyBatchNorm2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.nn.modules.conv.LazyConv2d'>\n",
      "<class 'torch.nn.modules.batchnorm.LazyBatchNorm2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.nn.modules.linear.LazyLinear'>\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "<class 'torch.nn.modules.rnn.LSTM'>\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "class Module:\n",
    "\n",
    "    def __init__(self, module: nn.Module, parent=None) -> None:\n",
    "        self.parent = parent\n",
    "        self.children: List[Module] = []\n",
    "        self.module = module\n",
    "    \n",
    "    def bfs(self):\n",
    "        queue = [self]\n",
    "        while len(queue) > 0:\n",
    "            queue.extend(queue[0].children)\n",
    "            yield queue.pop(0)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Module: {str(type(self.module))}'\n",
    "\n",
    "def create_graph(model: nn.Module,\n",
    "                 ignore: List[nn.Module]=[]\n",
    "    ) -> Union[Module, None]:\n",
    "\n",
    "    t = str(type(model))\n",
    "    if ('loss' in t)\\\n",
    "        or ('vocab' in t)\\\n",
    "        or (type(model) in ignore):\n",
    "        return None\n",
    "\n",
    "    children = []\n",
    "\n",
    "    root = Module(model)\n",
    "    for child in model.children():\n",
    "        if 'torch.nn.modules.container.Sequential' in str(type(child)):\n",
    "            for child_seq in child.children():\n",
    "                child_mod = create_graph(child_seq)\n",
    "                if child_mod is not None:\n",
    "                    root.children.append(child_mod)\n",
    "        else:\n",
    "            child_mod = create_graph(child)\n",
    "            if child_mod is not None:\n",
    "                root.children.append(child_mod)\n",
    "\n",
    "    return root\n",
    "\n",
    "graph = create_graph(model)\n",
    "\n",
    "for c in graph.bfs():\n",
    "    print(type(c.module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'torch.nn.modules.conv': ConvBlock,\n",
    "    'torch.nn.modules.batchnorm': NormBlock,\n",
    "    'torch.nn.modules.pooling': PoolBlock,\n",
    "    'torch.nn.modules.linear': LinearBlock,\n",
    "    'torch.nn.modules.rnn': LSTMBlock,\n",
    "    'torch.nn.modules.sparse.Embedding': EmbeddingBlock\n",
    "}\n",
    "\n",
    "it = iter(e.data.train_dataloader())\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Union\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from torch import Tensor, nn, Size\n",
    "\n",
    "from pycore.blocks import Block, Begin, ConvBlock, PoolBlock, Connection, End\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class Architecure:\n",
    "\n",
    "    @property\n",
    "    def inputs(self) -> List[str]:\n",
    "        return self._inputs\n",
    "    \n",
    "    @inputs.setter\n",
    "    def inputs(self, tensors: Union[Tensor, Tuple[Tensor]]) -> None:\n",
    "        if isinstance(tensors, Tensor):\n",
    "            tensors = [tensors]\n",
    "        \n",
    "        for i, t in enumerate(tensors):\n",
    "            if t.ndim > 3:\n",
    "                im_path = self.image_path.replace('{i}', str(i))\n",
    "                save_image(t[0], im_path)\n",
    "                self._blocks.append(InputBlock(i,\n",
    "                                               im_path,\n",
    "                                               to=(-3, 0, 10*i),\n",
    "                                               size=(self._size[1], self._size[2], 0)\n",
    "                                               ))\n",
    "                self._inputs.append(f'Input_{i}')\n",
    "            else:\n",
    "                self._blocks.append(LinearBlock(i,\n",
    "                                                to=(-3, 0, 10*i),\n",
    "                                                size=(self._size[0], 10, self._size[1])))\n",
    "                self._inputs.append(f'Linear_{i}')\n",
    "\n",
    "    def __init__(self,\n",
    "                 start_size=(2, 64, 64),\n",
    "                 pool_factor=0.8,\n",
    "                 conv_factor=0.8,\n",
    "                 image_path='./input_{i}.png') -> None:\n",
    "        self._blocks: List[Block] = [\n",
    "            Begin(),\n",
    "        ]\n",
    "        self._blocks_buffer: List[Tuple[nn.Module, Size]] = []\n",
    "        self._size = np.array(start_size)\n",
    "        self._tensor_size = None\n",
    "\n",
    "        self.image_path = image_path\n",
    "        self.pool_reduction = np.array([1, pool_factor, pool_factor])\n",
    "        self.conv_expansion = np.array([conv_factor, 1, 1])\n",
    "\n",
    "        self._inputs = []\n",
    "    \n",
    "    def __call__(self, module: nn.Module, x: Union[Tensor, Tuple[Tensor]]) -> None:\n",
    "        print(type(module), x[0].shape)\n",
    "        if len(self.inputs) == 0:\n",
    "            self.inputs = x\n",
    "\n",
    "        if self._tensor_size is None and isinstance(x, Tensor):\n",
    "            self._tensor_size = x.shape\n",
    "        \n",
    "        same_depth = torch.allclose()\n",
    "        \n",
    "        if 'pooling' not in str(type(module)) and\\\n",
    "           'Linear' not in str(type(module)) and\\\n",
    "            #TODO: test if tensor size changed\n",
    "            self._blocks_buffer.append((module, self._tensor_size))\n",
    "        else:\n",
    "            #TODO: add fused blocks correctly, calculate new sizes correctly (change in channels; and width, height by pooling)\n",
    "            for mod, tensor_size in self._blocks_buffer:\n",
    "                if tensor_size\n",
    "\n",
    "    def connect(self, block1, block2) -> None:\n",
    "        self._blocks.append(Connection(block1, block2))\n",
    "    \n",
    "    def finalize(self) -> str:\n",
    "        if not isinstance(self._blocks[-1], End):\n",
    "            self._blocks.append(End())\n",
    "        out = ''\n",
    "        for b in self._blocks:\n",
    "            out += f'\\n{b}'\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = Architecure()\n",
    "\n",
    "for c in graph.bfs():\n",
    "    if 'torch' in str(type(c.module)):\n",
    "        c.module.register_forward_pre_hook(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(1.9590, grad_fn=<NllLossBackward0>)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.validation_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\documentclass[border=8pt, multi, tikz]{standalone}\n",
      "\\usepackage{import}\n",
      "\\subimport{c:\\Users\\magnusson\\Documents\\projects\\pytorch2tikz\\pycore\\layers}{init}\n",
      "\\usetikzlibrary{positioning}\n",
      "\\usetikzlibrary{3d} %for including external image\n",
      "\n",
      "\\def\\ConvColor{rgb:yellow,5;red,2.5;white,5}\n",
      "\\def\\LinearColor{rgb:blue,5;red,2.5;white,5}\n",
      "\\def\\LstmColor{rgb:yellow,5;red,2.5;white,5}\n",
      "\\def\\ActivationColor{rgb:yellow,5;red,5;white,5}\n",
      "\\def\\PoolColor{rgb:red,1;black,0.3}\n",
      "\\def\\NormColor{rgb:red,1;black,0.3}\n",
      "\\def\\UnpoolColor{rgb:blue,2;green,1;black,0.3}\n",
      "\\def\\FcReluColor{rgb:blue,5;red,5;white,4}\n",
      "\\def\\SoftmaxColor{rgb:magenta,5;black,7}\n",
      "\\def\\SumColor{rgb:blue,5;green,15}\n",
      "\n",
      "\\newcommand{\\copymidarrow}{\\tikz \\draw[-Stealth,line width=0.8mm,draw={rgb:blue,4;red,1;green,1;black,3}] (-0.3,0) -- ++(0.3,0);}\n",
      "\n",
      "\\begin{document}\n",
      "\\begin{tikzpicture}\n",
      "\\tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\\edgecolor,opacity=0.7]\n",
      "\\tikzstyle{copyconnection}=[ultra thick,every node/.style={sloped,allow upside down},draw={rgb:blue,4;red,1;green,1;black,3},opacity=0.7]\n",
      "\n",
      "\n",
      "\\node[canvas is zy plane at x=0] (Input_0) at (-3, 0, 0) {\\includegraphics[width=64cm, height=64cm]{./input_0.png}};\n",
      "\n",
      "\n",
      "\\end{tikzpicture}\n",
      "\\end{document}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(arch.finalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "965a1e6d8ffbae966d3ad714d27549e31c062b7ec60fb2990dbc52b0370acbb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
